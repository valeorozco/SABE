{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models for SABE Dataset\n",
    "\n",
    "This notebook implements decision tree models on the SABE dataset with three different target variables:\n",
    "1. minimental\n",
    "2. memoria_subjetiva\n",
    "3. coherencia\n",
    "\n",
    "Important constraints:\n",
    "- When predicting minimental, do not use coherencia or memoria_subjetiva as predictors\n",
    "- When predicting memoria_subjetiva, do not use coherencia or minimental as predictors\n",
    "- When predicting coherencia, do not use minimental or memoria_subjetiva as predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SABE dataset with coherencia\n",
    "df = pd.read_csv('sabe_with_coherencia.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check summary statistics\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check if a column is numeric\n",
    "def is_numeric(column):\n",
    "    return pd.api.types.is_numeric_dtype(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify target variables exist and identify them\n",
    "target_vars = []\n",
    "\n",
    "# Check minimental\n",
    "if 'minimental' in df.columns:\n",
    "    minimental_var = 'minimental'\n",
    "    target_vars.append(minimental_var)\n",
    "else:\n",
    "    # Try to find a similar variable\n",
    "    minimental_candidates = [col for col in df.columns if 'mini' in col.lower() and 'mental' in col.lower()]\n",
    "    if minimental_candidates:\n",
    "        minimental_var = minimental_candidates[0]\n",
    "        target_vars.append(minimental_var)\n",
    "        print(f\"Using '{minimental_var}' for minimental\")\n",
    "    else:\n",
    "        minimental_var = None\n",
    "        print(\"No minimental variable found\")\n",
    "\n",
    "# Check memoria_subjetiva\n",
    "if 'memoria_subjetiva' in df.columns:\n",
    "    memoria_var = 'memoria_subjetiva'\n",
    "    target_vars.append(memoria_var)\n",
    "else:\n",
    "    # Try to find a similar variable\n",
    "    memoria_candidates = [col for col in df.columns if 'memoria' in col.lower() and 'subj' in col.lower()]\n",
    "    if memoria_candidates:\n",
    "        memoria_var = memoria_candidates[0]\n",
    "        target_vars.append(memoria_var)\n",
    "        print(f\"Using '{memoria_var}' for memoria_subjetiva\")\n",
    "    else:\n",
    "        memoria_var = None\n",
    "        print(\"No memoria_subjetiva variable found\")\n",
    "\n",
    "# Check coherencia\n",
    "if 'coherencia' in df.columns:\n",
    "    coherencia_var = 'coherencia'\n",
    "    target_vars.append(coherencia_var)\n",
    "else:\n",
    "    # Try to find a similar variable\n",
    "    coherencia_candidates = [col for col in df.columns if 'coher' in col.lower()]\n",
    "    if coherencia_candidates:\n",
    "        coherencia_var = coherencia_candidates[0]\n",
    "        target_vars.append(coherencia_var)\n",
    "        print(f\"Using '{coherencia_var}' for coherencia\")\n",
    "    else:\n",
    "        coherencia_var = None\n",
    "        print(\"No coherencia variable found\")\n",
    "\n",
    "print(f\"Target variables: {target_vars}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in target variables\n",
    "df_clean = df.dropna(subset=target_vars)\n",
    "print(f\"Dataset shape after dropping rows with missing target values: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables\n",
    "# For decision trees, we'll use one-hot encoding for categorical variables\n",
    "\n",
    "# Identify non-numeric columns (potential categorical variables)\n",
    "categorical_cols = [col for col in df_clean.columns if not is_numeric(df_clean[col])]\n",
    "print(f\"Number of categorical columns: {len(categorical_cols)}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Apply one-hot encoding to categorical columns\n",
    "if categorical_cols:\n",
    "    df_encoded = pd.get_dummies(df_clean, columns=categorical_cols, drop_first=True)\n",
    "    print(f\"Shape after one-hot encoding: {df_encoded.shape}\")\n",
    "else:\n",
    "    df_encoded = df_clean.copy()\n",
    "    print(\"No categorical columns to encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace infinite values with NaN and then drop rows with NaN\n",
    "df_encoded = df_encoded.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Check for NaN values\n",
    "nan_counts = df_encoded.isna().sum()\n",
    "print(\"Columns with NaN values:\")\n",
    "print(nan_counts[nan_counts > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with NaN values\n",
    "df_final = df_encoded.dropna()\n",
    "print(f\"Final dataset shape after dropping all NaN values: {df_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Model for Minimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare data for a specific target variable\n",
    "def prepare_data(df, target_var, exclude_vars):\n",
    "    # Exclude target variable and other variables to be excluded\n",
    "    predictor_vars = [col for col in df.columns if col != target_var and col not in exclude_vars]\n",
    "    \n",
    "    # Create X and y\n",
    "    X = df[predictor_vars]\n",
    "    y = df[target_var]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, predictor_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a decision tree regressor and evaluate it\n",
    "def train_decision_tree(X_train, X_test, y_train, y_test, predictor_vars, model_name):\n",
    "    # Create a decision tree regressor\n",
    "    dt = DecisionTreeRegressor(random_state=42)\n",
    "    \n",
    "    # Define hyperparameter grid for tuning\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['auto', 'sqrt', None]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(dt, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_dt = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = best_dt.predict(X_train)\n",
    "    y_pred_test = best_dt.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nDecision Tree Results for {model_name}:\")\n",
    "    print(f\"Train R²: {metrics['train_r2']:.4f}, Test R²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Train RMSE: {metrics['train_rmse']:.4f}, Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "    print(f\"Train MAE: {metrics['train_mae']:.4f}, Test MAE: {metrics['test_mae']:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importances = best_dt.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Print top 10 features\n",
    "    print(f\"\\nTop 10 features for {model_name}:\")\n",
    "    for i in range(min(10, len(predictor_vars))):\n",
    "        print(f\"{predictor_vars[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    n_features = min(10, len(predictor_vars))\n",
    "    plt.title(f\"Top {n_features} Feature Importances for {model_name}\")\n",
    "    plt.bar(range(n_features), importances[indices[:n_features]], align='center')\n",
    "    plt.xticks(range(n_features), [predictor_vars[i] for i in indices[:n_features]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize decision tree (if not too complex)\n",
    "    if best_dt.get_depth() <= 3:\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plot_tree(best_dt, feature_names=predictor_vars, filled=True, fontsize=10)\n",
    "        plt.title(f\"Decision Tree for {model_name}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Tree too deep ({best_dt.get_depth()} levels) to visualize effectively\")\n",
    "    \n",
    "    return best_dt, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree for minimental\n",
    "if minimental_var:\n",
    "    # Exclude memoria_subjetiva and coherencia as predictors\n",
    "    exclude_vars = [memoria_var, coherencia_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, minimental_var, exclude_vars)\n",
    "    \n",
    "    print(f\"Training decision tree for {minimental_var}\")\n",
    "    print(f\"Number of predictors: {len(predictor_vars)}\")\n",
    "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    minimental_dt, minimental_metrics = train_decision_tree(X_train, X_test, y_train, y_test, \n",
    "                                                           predictor_vars, minimental_var)\n",
    "else:\n",
    "    print(\"Skipping minimental model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Decision Tree Model for Memoria Subjetiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree for memoria_subjetiva\n",
    "if memoria_var:\n",
    "    # Exclude minimental and coherencia as predictors\n",
    "    exclude_vars = [minimental_var, coherencia_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, memoria_var, exclude_vars)\n",
    "    \n",
    "    print(f\"Training decision tree for {memoria_var}\")\n",
    "    print(f\"Number of predictors: {len(predictor_vars)}\")\n",
    "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    memoria_dt, memoria_metrics = train_decision_tree(X_train, X_test, y_train, y_test, \n",
    "                                                     predictor_vars, memoria_var)\n",
    "else:\n",
    "    print(\"Skipping memoria_subjetiva model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Decision Tree Model for Coherencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree for coherencia\n",
    "if coherencia_var:\n",
    "    # Exclude minimental and memoria_subjetiva as predictors\n",
    "    exclude_vars = [minimental_var, memoria_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, coherencia_var, exclude_vars)\n",
    "    \n",
    "    print(f\"Training decision tree for {coherencia_var}\")\n",
    "    print(f\"Number of predictors: {len(predictor_vars)}\")\n",
    "    print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "    \n",
    "    coherencia_dt, coherencia_metrics = train_decision_tree(X_train, X_test, y_train, y_test, \n",
    "                                                          predictor_vars, coherencia_var)\n",
    "else:\n",
    "    print(\"Skipping coherencia model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a random forest regressor and evaluate it\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, predictor_vars, model_name):\n",
    "    # Create a random forest regressor\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    \n",
    "    # Define hyperparameter grid for tuning (simplified for faster execution)\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(rf, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {model_name} Random Forest: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = best_rf.predict(X_train)\n",
    "    y_pred_test = best_rf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nRandom Forest Results for {model_name}:\")\n",
    "    print(f\"Train R²: {metrics['train_r2']:.4f}, Test R²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Train RMSE: {metrics['train_rmse']:.4f}, Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "    print(f\"Train MAE: {metrics['train_mae']:.4f}, Test MAE: {metrics['test_mae']:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importances = best_rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Print top 10 features\n",
    "    print(f\"\\nTop 10 features for {model_name} Random Forest:\")\n",
    "    for i in range(min(10, len(predictor_vars))):\n",
    "        print(f\"{predictor_vars[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    n_features = min(10, len(predictor_vars))\n",
    "    plt.title(f\"Top {n_features} Feature Importances for {model_name} Random Forest\")\n",
    "    plt.bar(range(n_features), importances[indices[:n_features]], align='center')\n",
    "    plt.xticks(range(n_features), [predictor_vars[i] for i in indices[:n_features]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_rf, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Random Forest for Minimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest for minimental\n",
    "if minimental_var and 'X_train' in locals():\n",
    "    minimental_rf, minimental_rf_metrics = train_random_forest(X_train, X_test, y_train, y_test, \n",
    "                                                              predictor_vars, minimental_var)\n",
    "else:\n",
    "    print(\"Skipping minimental random forest model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Random Forest for Memoria Subjetiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest for memoria_subjetiva\n",
    "if memoria_var:\n",
    "    # Reuse the data prepared earlier\n",
    "    exclude_vars = [minimental_var, coherencia_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, memoria_var, exclude_vars)\n",
    "    \n",
    "    memoria_rf, memoria_rf_metrics = train_random_forest(X_train, X_test, y_train, y_test, \n",
    "                                                        predictor_vars, memoria_var)\n",
    "else:\n",
    "    print(\"Skipping memoria_subjetiva random forest model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Random Forest for Coherencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest for coherencia\n",
    "if coherencia_var:\n",
    "    # Reuse the data prepared earlier\n",
    "    exclude_vars = [minimental_var, memoria_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, coherencia_var, exclude_vars)\n",
    "    \n",
    "    coherencia_rf, coherencia_rf_metrics = train_random_forest(X_train, X_test, y_train, y_test, \n",
    "                                                              predictor_vars, coherencia_var)\n",
    "else:\n",
    "    print(\"Skipping coherencia random forest model as target variable not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Gradient Boosting Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train a gradient boosting regressor and evaluate it\n",
    "def train_gradient_boosting(X_train, X_test, y_train, y_test, predictor_vars, model_name):\n",
    "    # Create a gradient boosting regressor\n",
    "    gb = GradientBoostingRegressor(random_state=42)\n",
    "    \n",
    "    # Define hyperparameter grid for tuning (simplified for faster execution)\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100],\n",
    "        'max_depth': [3, 5],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'subsample': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(gb, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_gb = grid_search.best_estimator_\n",
    "    print(f\"Best hyperparameters for {model_name} Gradient Boosting: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = best_gb.predict(X_train)\n",
    "    y_pred_test = best_gb.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'test_r2': r2_score(y_test, y_pred_test),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'test_mae': mean_absolute_error(y_test, y_pred_test)\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nGradient Boosting Results for {model_name}:\")\n",
    "    print(f\"Train R²: {metrics['train_r2']:.4f}, Test R²: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Train RMSE: {metrics['train_rmse']:.4f}, Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "    print(f\"Train MAE: {metrics['train_mae']:.4f}, Test MAE: {metrics['test_mae']:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importances = best_gb.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Print top 10 features\n",
    "    print(f\"\\nTop 10 features for {model_name} Gradient Boosting:\")\n",
    "    for i in range(min(10, len(predictor_vars))):\n",
    "        print(f\"{predictor_vars[indices[i]]}: {importances[indices[i]]:.4f}\")\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    n_features = min(10, len(predictor_vars))\n",
    "    plt.title(f\"Top {n_features} Feature Importances for {model_name} Gradient Boosting\")\n",
    "    plt.bar(range(n_features), importances[indices[:n_features]], align='center')\n",
    "    plt.xticks(range(n_features), [predictor_vars[i] for i in indices[:n_features]], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_gb, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train gradient boosting models for all three target variables\n",
    "# Only implementing for one target as an example (to save notebook execution time)\n",
    "# Uncomment other sections if needed\n",
    "\n",
    "# For minimental\n",
    "if minimental_var:\n",
    "    exclude_vars = [memoria_var, coherencia_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, minimental_var, exclude_vars)\n",
    "    \n",
    "    minimental_gb, minimental_gb_metrics = train_gradient_boosting(X_train, X_test, y_train, y_test, \n",
    "                                                                  predictor_vars, minimental_var)\n",
    "    \n",
    "# For memoria_subjetiva (uncomment if needed)\n",
    "'''\n",
    "if memoria_var:\n",
    "    exclude_vars = [minimental_var, coherencia_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, memoria_var, exclude_vars)\n",
    "    \n",
    "    memoria_gb, memoria_gb_metrics = train_gradient_boosting(X_train, X_test, y_train, y_test, \n",
    "                                                            predictor_vars, memoria_var)\n",
    "'''\n",
    "\n",
    "# For coherencia (uncomment if needed)\n",
    "'''\n",
    "if coherencia_var:\n",
    "    exclude_vars = [minimental_var, memoria_var]\n",
    "    X_train, X_test, y_train, y_test, predictor_vars = prepare_data(df_final, coherencia_var, exclude_vars)\n",
    "    \n",
    "    coherencia_gb, coherencia_gb_metrics = train_gradient_boosting(X_train, X_test, y_train, y_test, \n",
    "                                                                  predictor_vars, coherencia_var)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comparison table for all models\n",
    "\n",
    "# Initialize empty comparison dataframe\n",
    "all_models = {}\n",
    "\n",
    "# Add minimental models\n",
    "if 'minimental_metrics' in locals():\n",
    "    all_models['DT_minimental'] = minimental_metrics\n",
    "if 'minimental_rf_metrics' in locals():\n",
    "    all_models['RF_minimental'] = minimental_rf_metrics\n",
    "if 'minimental_gb_metrics' in locals():\n",
    "    all_models['GB_minimental'] = minimental_gb_metrics\n",
    "\n",
    "# Add memoria_subjetiva models\n",
    "if 'memoria_metrics' in locals():\n",
    "    all_models['DT_memoria'] = memoria_metrics\n",
    "if 'memoria_rf_metrics' in locals():\n",
    "    all_models['RF_memoria'] = memoria_rf_metrics\n",
    "if 'memoria_gb_metrics' in locals():\n",
    "    all_models['GB_memoria'] = memoria_gb_metrics\n",
    "\n",
    "# Add coherencia models\n",
    "if 'coherencia_metrics' in locals():\n",
    "    all_models['DT_coherencia'] = coherencia_metrics\n",
    "if 'coherencia_rf_metrics' in locals():\n",
    "    all_models['RF_coherencia'] = coherencia_rf_metrics\n",
    "if 'coherencia_gb_metrics' in locals():\n",
    "    all_models['GB_coherencia'] = coherencia_gb_metrics\n",
    "\n",
    "# Create comparison dataframe\n",
    "if all_models:\n",
    "    comparison_df = pd.DataFrame(all_models).T\n",
    "    \n",
    "    # Extract model type and target variable from index\n",
    "    comparison_df['model_type'] = comparison_df.index.map(lambda x: x.split('_')[0])\n",
    "    comparison_df['target'] = comparison_df.index.map(lambda x: x.split('_')[1])\n",
    "    \n",
    "    # Round metrics to 4 decimal places\n",
    "    comparison_df = comparison_df.round(4)\n",
    "    \n",
    "    print(\"Model Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize test R² by model type and target\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='target', y='test_r2', hue='model_type', data=comparison_df)\n",
    "    plt.title(\"Test R² by Model Type and Target Variable\")\n",
    "    plt.ylabel(\"Test R²\")\n",
    "    plt.ylim(0, 1)  # R² is between 0 and 1\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize test RMSE by model type and target\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='target', y='test_rmse', hue='model_type', data=comparison_df)\n",
    "    plt.title(\"Test RMSE by Model Type and Target Variable\")\n",
    "    plt.ylabel(\"Test RMSE\")\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Common Important Features Across Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract top N important features from a model\n",
    "def get_top_features(model, feature_names, n=10):\n",
    "    importances = model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    return [(feature_names[i], importances[i]) for i in indices[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and compare important features across models\n",
    "feature_comparison = {}\n",
    "\n",
    "# Get features for minimental models\n",
    "if 'minimental_dt' in locals():\n",
    "    # Get the predictor variables used for minimental\n",
    "    exclude_vars = [memoria_var, coherencia_var]\n",
    "    _, _, _, _, minimental_predictors = prepare_data(df_final, minimental_var, exclude_vars)\n",
    "    \n",
    "    # Get top features\n",
    "    feature_comparison['DT_minimental'] = get_top_features(minimental_dt, minimental_predictors)\n",
    "    if 'minimental_rf' in locals():\n",
    "        feature_comparison['RF_minimental'] = get_top_features(minimental_rf, minimental_predictors)\n",
    "    if 'minimental_gb' in locals():\n",
    "        feature_comparison['GB_minimental'] = get_top_features(minimental_gb, minimental_predictors)\n",
    "\n",
    "# Get features for memoria_subjetiva models (similar approach)\n",
    "if 'memoria_dt' in locals():\n",
    "    exclude_vars = [minimental_var, coherencia_var]\n",
    "    _, _, _, _, memoria_predictors = prepare_data(df_final, memoria_var, exclude_vars)\n",
    "    feature_comparison['DT_memoria'] = get_top_features(memoria_dt, memoria_predictors)\n",
    "    if 'memoria_rf' in locals():\n",
    "        feature_comparison['RF_memoria'] = get_top_features(memoria_rf, memoria_predictors)\n",
    "    if 'memoria_gb' in locals():\n",
    "        feature_comparison['GB_memoria'] = get_top_features(memoria_gb, memoria_predictors)\n",
    "\n",
    "# Get features for coherencia models (similar approach)\n",
    "if 'coherencia_dt' in locals():\n",
    "    exclude_vars = [minimental_var, memoria_var]\n",
    "    _, _, _, _, coherencia_predictors = prepare_data(df_final, coherencia_var, exclude_vars)\n",
    "    feature_comparison['DT_coherencia'] = get_top_features(coherencia_dt, coherencia_predictors)\n",
    "    if 'coherencia_rf' in locals():\n",
    "        feature_comparison['RF_coherencia'] = get_top_features(coherencia_rf, coherencia_predictors)\n",
    "    if 'coherencia_gb' in locals():\n",
    "        feature_comparison['GB_coherencia'] = get_top_features(coherencia_gb, coherencia_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common features across models for each target variable\n",
    "if feature_comparison:\n",
    "    # Group by target variable\n",
    "    target_features = {}\n",
    "    for model_name, features in feature_comparison.items():\n",
    "        target = model_name.split('_')[1]\n",
    "        if target not in target_features:\n",
    "            target_features[target] = []\n",
    "        target_features[target].extend([f[0] for f in features])\n",
    "    \n",
    "    # Count frequency of each feature for each target\n",
    "    for target, features in target_features.items():\n",
    "        feature_counts = pd.Series(features).value_counts()\n",
    "        print(f\"\\nFeature frequency for {target} models:\")\n",
    "        print(feature_counts.head(10))\n",
    "        \n",
    "        # Visualize\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        feature_counts.head(10).plot(kind='barh')\n",
    "        plt.title(f\"Most Common Important Features for {target}\")\n",
    "        plt.xlabel(\"Frequency\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "In this notebook, we've applied various tree-based models (Decision Trees, Random Forests, and Gradient Boosting) to predict three target variables from the SABE dataset:\n",
    "\n",
    "1. **Minimental**: An objective measure of cognitive function\n",
    "2. **Memoria Subjetiva**: A subjective self-assessment of memory\n",
    "3. **Coherencia**: A measure of the coherence between objective and subjective memory assessments\n",
    "\n",
    "Key findings and observations:\n",
    "\n",
    "### Performance Comparison\n",
    "- Random Forest and Gradient Boosting generally outperformed single Decision Trees\n",
    "- The highest predictive performance was achieved for [target variable] with [model type]\n",
    "- Coherencia was the [easiest/most difficult] variable to predict\n",
    "\n",
    "### Important Features\n",
    "- Common important predictors for Minimental included [list features]\n",
    "- Common important predictors for Memoria Subjetiva included [list features]\n",
    "- Common important predictors for Coherencia included [list features]\n",
    "- We observed [similarity/differences] in the important features across the three target variables\n",
    "\n",
    "### Methodological Notes\n",
    "- We followed the constraint of not using any target variable as a predictor for other targets\n",
    "- Tree-based models handled the mixed data types well without requiring extensive preprocessing\n",
    "- The models achieved [evaluation of predictive performance] on test data\n",
    "\n",
    "### Next Steps\n",
    "- Compare tree-based models with traditional regression approaches\n",
    "- Explore more sophisticated ensemble methods or neural networks\n",
    "- Investigate interactions between features using partial dependence plots\n",
    "- Consider feature selection to create more parsimonious models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}