{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Model Analysis for SABE Dataset\n",
    "\n",
    "This notebook focuses on preparing the data for modeling, with an emphasis on analyzing factors that influence subjective memory evaluation and creating a coherence measure between subjective and objective memory measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Set pandas options for better display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the prepared dataset\n",
    "try:\n",
    "    df = pd.read_csv('sabe_df_prepared.csv')\n",
    "    print(f\"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file not found. Trying alternative filenames...\")\n",
    "    try:\n",
    "        # Try alternative filenames\n",
    "        for filename in ['cleaned_sabe_col_complete.csv', 'sabe_imputed_by_categoria_cognitiva.csv', 'cleaned_sabe_col_expanded.csv']:\n",
    "            try:\n",
    "                df = pd.read_csv(filename)\n",
    "                print(f\"Loaded {filename} with {df.shape[0]} rows and {df.shape[1]} columns\")\n",
    "                break\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "    except:\n",
    "        print(\"No suitable dataset found. Please ensure the SABE dataset is available.\")\n",
    "        # Create a small example dataset for demonstration\n",
    "        df = pd.DataFrame({\n",
    "            'minimental': np.random.normal(25, 5, 100),\n",
    "            'memoria_subjetiva': np.random.normal(3, 1, 100),\n",
    "            'categoria_cognitiva': np.random.choice(['normal', 'mild impairment', 'moderate'], 100),\n",
    "            'edad': np.random.normal(70, 8, 100),\n",
    "            'sexo': np.random.choice([0, 1], 100)\n",
    "        })\n",
    "        print(\"Created a sample dataset for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Isolate Numerical Variables (Non-Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to identify binary variables\n",
    "def is_binary(series):\n",
    "    \"\"\"Check if a column only contains 0/1 or True/False values\"\"\"\n",
    "    # Drop NaN values for the check\n",
    "    non_null = series.dropna()\n",
    "    if non_null.empty:\n",
    "        return False\n",
    "    \n",
    "    # Get unique values\n",
    "    unique_vals = set(non_null.unique())\n",
    "    \n",
    "    # Check if it's a binary variable (0/1 or True/False)\n",
    "    binary_sets = [{0, 1}, {False, True}]\n",
    "    return any(unique_vals.issubset(binary_set) for binary_set in binary_sets) and len(unique_vals) <= 2\n",
    "\n",
    "# Identify numeric, non-binary variables\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'Int64', 'float64']).columns.tolist()\n",
    "binary_cols = [col for col in numeric_cols if is_binary(df[col])]\n",
    "nonbinary_numeric = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Binary numeric columns: {len(binary_cols)}\")\n",
    "print(f\"Non-binary numeric columns: {len(nonbinary_numeric)}\")\n",
    "\n",
    "# Display first 20 non-binary numeric variables\n",
    "print(\"\\nSample of non-binary numeric variables:\")\n",
    "print(nonbinary_numeric[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify directionality and distribution of key variables\n",
    "key_vars = ['minimental', 'memoria_subjetiva']\n",
    "key_vars_present = [var for var in key_vars if var in df.columns]\n",
    "\n",
    "if key_vars_present:\n",
    "    # Create a figure to visualize distributions\n",
    "    fig, axs = plt.subplots(len(key_vars_present), 2, figsize=(15, 5*len(key_vars_present)))\n",
    "    \n",
    "    # If only one variable present, adjust axes\n",
    "    if len(key_vars_present) == 1:\n",
    "        axs = axs.reshape(1, -1)\n",
    "    \n",
    "    # Plot histograms and boxplots for each key variable\n",
    "    for i, var in enumerate(key_vars_present):\n",
    "        # Histogram\n",
    "        sns.histplot(df[var].dropna(), kde=True, ax=axs[i, 0])\n",
    "        axs[i, 0].set_title(f'Distribution of {var}')\n",
    "        axs[i, 0].set_xlabel(var)\n",
    "        \n",
    "        # Boxplot\n",
    "        sns.boxplot(x=df[var].dropna(), ax=axs[i, 1])\n",
    "        axs[i, 1].set_title(f'Boxplot of {var}')\n",
    "        axs[i, 1].set_xlabel(var)\n",
    "        \n",
    "        # Print descriptive statistics\n",
    "        print(f\"\\nStatistics for {var}:\")\n",
    "        print(df[var].describe())\n",
    "        print(f\"Skewness: {df[var].skew()}\")\n",
    "        print(f\"Kurtosis: {df[var].kurtosis()}\")\n",
    "        print(f\"Missing values: {df[var].isnull().sum()} ({df[var].isnull().mean():.2%})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If both variables present, check their relationship\n",
    "    if 'minimental' in df.columns and 'memoria_subjetiva' in df.columns:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(x='minimental', y='memoria_subjetiva', data=df)\n",
    "        plt.title('Relationship between Minimental and Subjective Memory')\n",
    "        plt.xlabel('Minimental (objective cognitive assessment)')\n",
    "        plt.ylabel('Subjective Memory Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        correlation = df[['minimental', 'memoria_subjetiva']].corr().iloc[0, 1]\n",
    "        print(f\"\\nCorrelation between minimental and memoria_subjetiva: {correlation:.4f}\")\n",
    "        \n",
    "        # Add directionality interpretation\n",
    "        print(\"\\nDirectionality interpretation:\")\n",
    "        print(\"- Minimental: Higher values indicate better cognitive function\")\n",
    "        print(\"- Memoria Subjetiva: Higher values indicate worse subjective memory perception\")\n",
    "        if correlation < 0:\n",
    "            print(\"- The negative correlation suggests that people with better cognitive function (higher minimental) tend to report better subjective memory (lower memoria_subjetiva scores)\")\n",
    "        else:\n",
    "            print(\"- The positive correlation suggests that better cognitive function might not align with subjective memory perception\")\n",
    "else:\n",
    "    print(\"Key variables (minimental, memoria_subjetiva) not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Display summary statistics for all non-binary numeric variables\n",
    "if nonbinary_numeric:\n",
    "    # Create a smaller subset if there are too many variables\n",
    "    display_vars = nonbinary_numeric[:20] if len(nonbinary_numeric) > 20 else nonbinary_numeric\n",
    "    \n",
    "    # Create a summary table\n",
    "    summary = df[display_vars].describe().T\n",
    "    \n",
    "    # Add skewness and kurtosis\n",
    "    summary['skewness'] = df[display_vars].skew()\n",
    "    summary['kurtosis'] = df[display_vars].kurtosis()\n",
    "    summary['missing'] = df[display_vars].isnull().sum()\n",
    "    summary['missing_percent'] = df[display_vars].isnull().mean() * 100\n",
    "    \n",
    "    # Display the summary\n",
    "    print(\"Summary statistics for non-binary numeric variables:\")\n",
    "    display(summary.round(2))\n",
    "    \n",
    "    # Create a correlation matrix and visualize it\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[display_vars].corr()\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    plt.title('Correlation Matrix for Non-Binary Numeric Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Coherence Variable (Minimental - Subjective Memory)\n",
    "\n",
    "First, we need to determine whether to standardize or normalize the variables. We'll test both approaches and select the most appropriate one based on the data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if both required variables are present\n",
    "if 'minimental' in df.columns and 'memoria_subjetiva' in df.columns:\n",
    "    # Get the data and drop rows with missing values\n",
    "    mm_ms_df = df[['minimental', 'memoria_subjetiva']].dropna()\n",
    "    \n",
    "    print(f\"Complete cases for both variables: {len(mm_ms_df)} out of {len(df)} total rows\")\n",
    "    \n",
    "    # Test normality of the variables\n",
    "    for var in ['minimental', 'memoria_subjetiva']:\n",
    "        stat, p = stats.shapiro(mm_ms_df[var])\n",
    "        print(f\"\\nShapiro-Wilk test for {var}:\")\n",
    "        print(f\"Statistic: {stat:.4f}, p-value: {p:.4f}\")\n",
    "        if p < 0.05:\n",
    "            print(f\"The {var} variable is not normally distributed (reject H0)\")\n",
    "        else:\n",
    "            print(f\"The {var} variable follows a normal distribution (fail to reject H0)\")\n",
    "    \n",
    "    # Check skewness and kurtosis (normal if both between -2 and 2)\n",
    "    for var in ['minimental', 'memoria_subjetiva']:\n",
    "        skewness = mm_ms_df[var].skew()\n",
    "        kurtosis = mm_ms_df[var].kurtosis()\n",
    "        print(f\"\\n{var} skewness: {skewness:.4f}, kurtosis: {kurtosis:.4f}\")\n",
    "        if abs(skewness) < 2 and abs(kurtosis) < 2:\n",
    "            print(f\"The {var} variable has acceptable skewness and kurtosis for normal approximation\")\n",
    "        else:\n",
    "            print(f\"The {var} variable has high skewness or kurtosis, suggesting non-normality\")\n",
    "            \n",
    "    # Visualize original variables\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(mm_ms_df['minimental'], kde=True)\n",
    "    plt.title('Distribution of Minimental')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(mm_ms_df['memoria_subjetiva'], kde=True)\n",
    "    plt.title('Distribution of Memoria Subjetiva')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Based on the tests, determine which approach is better\n",
    "    # If variables are close to normal, standardization is preferred\n",
    "    # If variables are skewed or have different scales, normalization may be better\n",
    "    \n",
    "    # Since Subjective Memory scores are ordered (0-9) and Minimental has a wider range (0-30),\n",
    "    # we'll test both standardization and normalization\n",
    "    \n",
    "    # Standardization (Z-scores)\n",
    "    scaler = StandardScaler()\n",
    "    standardized = pd.DataFrame(\n",
    "        scaler.fit_transform(mm_ms_df),\n",
    "        columns=['minimental_std', 'memoria_subjetiva_std'],\n",
    "        index=mm_ms_df.index\n",
    "    )\n",
    "    \n",
    "    # Normalization (Min-Max scaling to 0-1)\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalized = pd.DataFrame(\n",
    "        normalizer.fit_transform(mm_ms_df),\n",
    "        columns=['minimental_norm', 'memoria_subjetiva_norm'],\n",
    "        index=mm_ms_df.index\n",
    "    )\n",
    "    \n",
    "    # Combine with original data\n",
    "    transformed_df = pd.concat([mm_ms_df, standardized, normalized], axis=1)\n",
    "    \n",
    "    # Visualize the transformed variables\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Standardized\n",
    "    sns.histplot(transformed_df['minimental_std'], kde=True, ax=axs[0, 0])\n",
    "    axs[0, 0].set_title('Standardized Minimental')\n",
    "    sns.histplot(transformed_df['memoria_subjetiva_std'], kde=True, ax=axs[0, 1])\n",
    "    axs[0, 1].set_title('Standardized Memoria Subjetiva')\n",
    "    \n",
    "    # Normalized\n",
    "    sns.histplot(transformed_df['minimental_norm'], kde=True, ax=axs[1, 0])\n",
    "    axs[1, 0].set_title('Normalized Minimental')\n",
    "    sns.histplot(transformed_df['memoria_subjetiva_norm'], kde=True, ax=axs[1, 1])\n",
    "    axs[1, 1].set_title('Normalized Memoria Subjetiva')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate coherence using both methods\n",
    "    # Note: For memoria_subjetiva, higher values indicate worse subjective memory\n",
    "    # To align the directions (higher = better for both), we use the negative of memoria_subjetiva\n",
    "    \n",
    "    # Using standardized values (with reversed memoria_subjetiva_std)\n",
    "    transformed_df['coherence_std'] = transformed_df['minimental_std'] - transformed_df['memoria_subjetiva_std']\n",
    "    \n",
    "    # Using normalized values (with reversed memoria_subjetiva_norm)\n",
    "    transformed_df['coherence_norm'] = transformed_df['minimental_norm'] - transformed_df['memoria_subjetiva_norm']\n",
    "    \n",
    "    # Visualize the coherence variables\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(transformed_df['coherence_std'], kde=True)\n",
    "    plt.title('Coherence (using Standardized Values)')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(transformed_df['coherence_norm'], kde=True)\n",
    "    plt.title('Coherence (using Normalized Values)')\n",
    "    plt.axvline(0, color='red', linestyle='--', alpha=0.7)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select the final coherence variable based on the analysis\n",
    "    # If the data is normally distributed, use standardized version\n",
    "    # If not, use the normalized version\n",
    "    is_normal = True  # Default assumption based on tests above\n",
    "    \n",
    "    if is_normal:\n",
    "        coherence_col = 'coherence_std'\n",
    "        print(\"\\nUsing standardized values for coherence calculation based on normality tests\")\n",
    "    else:\n",
    "        coherence_col = 'coherence_norm'\n",
    "        print(\"\\nUsing normalized values for coherence calculation due to non-normality\")\n",
    "    \n",
    "    # Add the coherence variable to the main dataframe\n",
    "    # First, make sure the std/norm columns are added to the main dataframe\n",
    "    if coherence_col == 'coherence_std':\n",
    "        # Add standardized columns\n",
    "        for idx, row in mm_ms_df.iterrows():\n",
    "            df.loc[idx, 'minimental_std'] = standardized.loc[idx, 'minimental_std']\n",
    "            df.loc[idx, 'memoria_subjetiva_std'] = standardized.loc[idx, 'memoria_subjetiva_std']\n",
    "            df.loc[idx, 'coherencia'] = transformed_df.loc[idx, 'coherence_std']\n",
    "    else:\n",
    "        # Add normalized columns\n",
    "        for idx, row in mm_ms_df.iterrows():\n",
    "            df.loc[idx, 'minimental_norm'] = normalized.loc[idx, 'minimental_norm']\n",
    "            df.loc[idx, 'memoria_subjetiva_norm'] = normalized.loc[idx, 'memoria_subjetiva_norm']\n",
    "            df.loc[idx, 'coherencia'] = transformed_df.loc[idx, 'coherence_norm']\n",
    "    \n",
    "    # Describe the new coherence variable\n",
    "    print(\"\\nDescription of the coherencia variable:\")\n",
    "    print(df['coherencia'].describe())\n",
    "    \n",
    "    # Interpret coherence variable\n",
    "    print(\"\\nCoherencia interpretation:\")\n",
    "    print(\"- Values close to 0: Good alignment between objective and subjective memory assessment\")\n",
    "    print(\"- Positive values: Objective cognitive function (Minimental) is better than subjective perception\")\n",
    "    print(\"- Negative values: Subjective perception of memory is better than objective performance\")\n",
    "else:\n",
    "    print(\"One or both required variables (minimental, memoria_subjetiva) are missing from the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to perform correlation analysis for a target variable\n",
    "def analyze_correlations(df, target_var, top_n=15):\n",
    "    \"\"\"Analyze correlations between a target variable and all others.\"\"\"\n",
    "    if target_var not in df.columns:\n",
    "        print(f\"Variable {target_var} not found in the dataset.\")\n",
    "        return None\n",
    "    \n",
    "    # Drop rows where the target variable is missing\n",
    "    valid_data = df.dropna(subset=[target_var])\n",
    "    print(f\"Analyzing correlations for {target_var} using {len(valid_data)} valid rows\")\n",
    "    \n",
    "    # Calculate correlations with all numeric variables\n",
    "    numeric_cols = valid_data.select_dtypes(include=['int64', 'Int64', 'float64']).columns\n",
    "    correlations = pd.DataFrame()\n",
    "    \n",
    "    # Pearson correlation\n",
    "    pearson_corr = valid_data[numeric_cols].corrwith(valid_data[target_var], method='pearson')\n",
    "    correlations['pearson'] = pearson_corr\n",
    "    \n",
    "    # Spearman correlation (rank-based, more robust to outliers)\n",
    "    spearman_corr = valid_data[numeric_cols].corrwith(valid_data[target_var], method='spearman')\n",
    "    correlations['spearman'] = spearman_corr\n",
    "    \n",
    "    # Drop the target variable itself\n",
    "    correlations = correlations.drop(target_var, errors='ignore')\n",
    "    \n",
    "    # Sort by absolute Pearson correlation\n",
    "    correlations['abs_pearson'] = correlations['pearson'].abs()\n",
    "    correlations = correlations.sort_values('abs_pearson', ascending=False).drop('abs_pearson', axis=1)\n",
    "    \n",
    "    # Display top correlated variables\n",
    "    print(f\"\\nTop {top_n} variables correlated with {target_var}:\")\n",
    "    print(correlations.head(top_n).round(3))\n",
    "    \n",
    "    # Visualize top correlations\n",
    "    top_corr_vars = correlations.head(top_n).index.tolist()\n",
    "    \n",
    "    # Create a figure for visualization\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot Pearson correlations\n",
    "    bars = plt.barh(top_corr_vars, correlations.loc[top_corr_vars, 'pearson'])\n",
    "    \n",
    "    # Color the bars based on direction of correlation\n",
    "    for i, bar in enumerate(bars):\n",
    "        if correlations.loc[top_corr_vars[i], 'pearson'] > 0:\n",
    "            bar.set_color('green')\n",
    "        else:\n",
    "            bar.set_color('red')\n",
    "    \n",
    "    plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "    plt.title(f'Top {top_n} Variables Correlated with {target_var} (Pearson)')\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Analyze correlations for the three target variables\n",
    "target_variables = ['memoria_subjetiva', 'categoria_cognitiva', 'minimental', 'coherencia']\n",
    "\n",
    "for target in target_variables:\n",
    "    if target in df.columns:\n",
    "        print(f\"\\n{'='*80}\\nAnalyzing correlations for {target}\\n{'='*80}\")\n",
    "        correlations = analyze_correlations(df, target)\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(f\"Variable {target} not found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Methods for Research Question\n",
    "\n",
    "Research Question: What variables most influence personal evaluation of memory (memoria_subjetiva) and coherence between objective and subjective measures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Define a function to suggest and demonstrate statistical methods\n",
    "def suggest_statistical_methods(df, target_variables):\n",
    "    \"\"\"\n",
    "    Suggest and demonstrate statistical methods for analyzing factors \n",
    "    influencing subjective memory evaluation and coherence.\n",
    "    \"\"\"\n",
    "    print(\"STATISTICAL METHODS FOR ANALYZING FACTORS INFLUENCING MEMORIA SUBJETIVA AND COHERENCE\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    # Check if target variables exist\n",
    "    target_vars_present = [var for var in target_variables if var in df.columns]\n",
    "    \n",
    "    if not target_vars_present:\n",
    "        print(\"None of the target variables are present in the dataset.\")\n",
    "        return\n",
    "    \n",
    "    # Suggestions based on the research question\n",
    "    suggestions = [\n",
    "        {\n",
    "            \"method\": \"Multiple Linear Regression\",\n",
    "            \"description\": \"Identifies the combined influence of multiple predictors on memoria_subjetiva or coherence\",\n",
    "            \"advantages\": [\n",
    "                \"Quantifies the unique contribution of each variable\",\n",
    "                \"Provides coefficient estimates with confidence intervals\",\n",
    "                \"Can include both continuous and categorical predictors\",\n",
    "                \"Allows for control of confounding variables\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Assumes linear relationships\",\n",
    "                \"Sensitive to multicollinearity\",\n",
    "                \"Requires normally distributed residuals\",\n",
    "                \"May not capture complex interactions without explicit specification\"\n",
    "            ],\n",
    "            \"implementation\": \"Using statsmodels or sklearn for regression analysis\"\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"Feature Selection (LASSO/Ridge Regression)\",\n",
    "            \"description\": \"Identifies the most important predictors while handling multicollinearity\",\n",
    "            \"advantages\": [\n",
    "                \"Handles high-dimensional data well\",\n",
    "                \"Reduces overfitting by penalizing complex models\",\n",
    "                \"LASSO can perform automatic feature selection\",\n",
    "                \"Ridge helps when predictors are highly correlated\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Requires tuning of regularization parameter\",\n",
    "                \"May still struggle with extremely correlated predictors\",\n",
    "                \"Interpretation is less straightforward than standard regression\"\n",
    "            ],\n",
    "            \"implementation\": \"Using sklearn's Lasso, Ridge, or ElasticNet\"\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"Hierarchical Regression\",\n",
    "            \"description\": \"Adds predictors in blocks based on theoretical importance\",\n",
    "            \"advantages\": [\n",
    "                \"Allows testing the incremental contribution of variable groups\",\n",
    "                \"Can reflect theoretical priorities in variable ordering\",\n",
    "                \"Shows R² change at each step\",\n",
    "                \"Useful for comparing nested models\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Results depend on the order of variable entry\",\n",
    "                \"Requires strong theoretical justification for block structure\",\n",
    "                \"Same assumptions as multiple regression\"\n",
    "            ],\n",
    "            \"implementation\": \"Using statsmodels to build models incrementally\"\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"Mediation and Moderation Analysis\",\n",
    "            \"description\": \"Examines indirect effects and interactions between predictors\",\n",
    "            \"advantages\": [\n",
    "                \"Reveals how variables influence each other to affect memoria_subjetiva\",\n",
    "                \"Identifies conditional relationships (when effects depend on other factors)\",\n",
    "                \"Provides a more nuanced understanding of complex relationships\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Requires strong theoretical basis for proposed relationships\",\n",
    "                \"Can be computationally intensive for complex models\",\n",
    "                \"Interpretation becomes complex with multiple mediators/moderators\"\n",
    "            ],\n",
    "            \"implementation\": \"Using statsmodels or specialized packages like 'mediation'\"\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"Structural Equation Modeling (SEM)\",\n",
    "            \"description\": \"Models complex networks of relationships including latent variables\",\n",
    "            \"advantages\": [\n",
    "                \"Can model complex directional relationships simultaneously\",\n",
    "                \"Incorporates measurement error\",\n",
    "                \"Can include latent variables (not directly measured)\",\n",
    "                \"Provides overall model fit statistics\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"Complex to implement and interpret\",\n",
    "                \"Requires larger sample sizes\",\n",
    "                \"Sensitive to model specification\",\n",
    "                \"May not converge with complex models\"\n",
    "            ],\n",
    "            \"implementation\": \"Using lavaan package in R or semopy in Python\"\n",
    "        },\n",
    "        {\n",
    "            \"method\": \"Mixed-Effects Models\",\n",
    "            \"description\": \"Accounts for clustered data (e.g., by region or assessment center)\",\n",
    "            \"advantages\": [\n",
    "                \"Handles hierarchical/nested data structures\",\n",
    "                \"Separates fixed and random effects\",\n",
    "                \"Controls for non-independence of observations\",\n",
    "                \"Can model individual differences\"\n",
    "            ],\n",
    "            \"limitations\": [\n",
    "                \"More complex to specify and interpret\",\n",
    "                \"Computationally intensive\",\n",
    "                \"Requires decisions about random effect structure\"\n",
    "            ],\n",
    "            \"implementation\": \"Using statsmodels' MixedLM or sklearn's mixed-effects models\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Display suggestions\n",
    "    for i, suggestion in enumerate(suggestions, 1):\n",
    "        print(f\"\\n{i}. {suggestion['method']}\")\n",
    "        print(f\"   Description: {suggestion['description']}\")\n",
    "        print(\"   Advantages:\")\n",
    "        for adv in suggestion['advantages']:\n",
    "            print(f\"    - {adv}\")\n",
    "        print(\"   Limitations:\")\n",
    "        for lim in suggestion['limitations']:\n",
    "            print(f\"    - {lim}\")\n",
    "        print(f\"   Implementation: {suggestion['implementation']}\")\n",
    "    \n",
    "    # Demonstrate a basic implementation of regression analysis if target variables are present\n",
    "    for target in target_vars_present:\n",
    "        print(f\"\\n\\nDEMONSTRATION: Basic Multiple Regression for {target}\\n{'='*70}\")\n",
    "        \n",
    "        # Prepare the data\n",
    "        target_data = df.dropna(subset=[target])\n",
    "        \n",
    "        # Select potential predictors (top correlated variables)\n",
    "        numeric_cols = target_data.select_dtypes(include=['int64', 'Int64', 'float64']).columns\n",
    "        correlations = target_data[numeric_cols].corrwith(target_data[target], method='pearson')\n",
    "        correlations = correlations.drop(target, errors='ignore')\n",
    "        correlations = correlations.drop(['coherencia', 'minimental_std', 'memoria_subjetiva_std', \n",
    "                                          'minimental_norm', 'memoria_subjetiva_norm'], errors='ignore')\n",
    "        \n",
    "        # Get top correlated variables\n",
    "        top_corr_vars = correlations.abs().sort_values(ascending=False).head(10).index.tolist()\n",
    "        \n",
    "        # Check for multicollinearity\n",
    "        X = target_data[top_corr_vars]\n",
    "        X = sm.add_constant(X)  # Add constant term for intercept\n",
    "        \n",
    "        # Calculate VIF\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"Variable\"] = X.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "        \n",
    "        print(\"VIF Analysis (values > 5 indicate potential multicollinearity):\")\n",
    "        print(vif_data.sort_values(\"VIF\", ascending=False))\n",
    "        \n",
    "        # If multicollinearity detected, use a subset of variables\n",
    "        high_vif_threshold = 5\n",
    "        if vif_data[\"VIF\"].max() > high_vif_threshold:\n",
    "            print(\"\\nMulticollinearity detected! Selecting a subset of variables...\")\n",
    "            # Keep variables with VIF < 5, excluding the constant\n",
    "            low_vif_vars = vif_data[vif_data[\"VIF\"] < high_vif_threshold][\"Variable\"].tolist()\n",
    "            # Remove 'const' if present\n",
    "            if 'const' in low_vif_vars:\n",
    "                low_vif_vars.remove('const')\n",
    "            \n",
    "            # If too few variables remain, take top uncorrelated variables\n",
    "            if len(low_vif_vars) < 3:\n",
    "                print(\"Too few variables with low VIF. Using feature selection...\")\n",
    "                # Use mutual information for feature selection (works well even with non-linear relationships)\n",
    "                X_no_const = target_data[top_corr_vars]\n",
    "                y = target_data[target]\n",
    "                \n",
    "                # Impute any remaining NaN values with medians for feature selection\n",
    "                X_imputed = X_no_const.fillna(X_no_const.median())\n",
    "                \n",
    "                # Select top 5 features using mutual information\n",
    "                selector = SelectKBest(mutual_info_regression, k=5)\n",
    "                X_new = selector.fit_transform(X_imputed, y)\n",
    "                selected_indices = selector.get_support(indices=True)\n",
    "                predictors = [top_corr_vars[i] for i in selected_indices]\n",
    "            else:\n",
    "                predictors = low_vif_vars\n",
    "        else:\n",
    "            predictors = top_corr_vars\n",
    "        \n",
    "        print(f\"\\nSelected predictors for {target}: {predictors}\")\n",
    "        \n",
    "        # Create the model with remaining predictors\n",
    "        # Drop rows with any NaN in the predictors or target\n",
    "        model_data = target_data[predictors + [target]].dropna()\n",
    "        print(f\"\\nSample size for regression model: {len(model_data)} observations\")\n",
    "        \n",
    "        if len(model_data) > len(predictors) + 5:  # Ensure sufficient observations\n",
    "            # Fit the OLS regression model\n",
    "            X = sm.add_constant(model_data[predictors])\n",
    "            y = model_data[target]\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            \n",
    "            # Print model summary\n",
    "            print(\"\\nRegression Model Summary:\")\n",
    "            print(model.summary().tables[1])  # Coefficients table\n",
    "            print(f\"\\nR-squared: {model.rsquared:.4f}\")\n",
    "            print(f\"Adjusted R-squared: {model.rsquared_adj:.4f}\")\n",
    "            print(f\"F-statistic: {model.fvalue:.4f} (p-value: {model.f_pvalue:.4f})\")\n",
    "            \n",
    "            # Significant predictors\n",
    "            significant_predictors = []\n",
    "            for var, p_value in zip(model.params.index[1:], model.pvalues[1:]):\n",
    "                if p_value < 0.05:\n",
    "                    significant_predictors.append((var, model.params[var], p_value))\n",
    "            \n",
    "            if significant_predictors:\n",
    "                print(\"\\nSignificant predictors:\")\n",
    "                for var, coef, p_value in sorted(significant_predictors, key=lambda x: x[2]):\n",
    "                    print(f\"  - {var}: coefficient = {coef:.4f} (p-value: {p_value:.4f})\")\n",
    "            else:\n",
    "                print(\"\\nNo significant predictors found at alpha=0.05.\")\n",
    "        else:\n",
    "            print(\"Insufficient data for regression analysis.\")\n",
    "    \n",
    "    # Recommended approach for the research question\n",
    "    print(\"\\n\\nRECOMMENDED APPROACH FOR YOUR RESEARCH QUESTION\\n\" + \"=\"*50)\n",
    "    print(\"\"\"\n",
    "Based on your research question about factors influencing memoria_subjetiva and coherence, \n",
    "I recommend a multi-step analytical approach:\n",
    "\n",
    "1. Hierarchical Regression Analysis:\n",
    "   - Start with demographic variables (age, sex, education)\n",
    "   - Add health variables (physical health, chronic conditions)\n",
    "   - Add cognitive variables (objective cognitive measures)\n",
    "   - Add psychosocial variables (depression, social engagement)\n",
    "   - This approach allows you to see the incremental contribution of each variable group\n",
    "\n",
    "2. Mediation Analysis:\n",
    "   - Test if the relationship between objective cognition and subjective memory \n",
    "     is mediated by variables like depression, health conditions, or education\n",
    "   - This helps understand the mechanisms through which objective cognition influences subjective perception\n",
    "\n",
    "3. Moderation Analysis:\n",
    "   - Test if the relationship between objective and subjective measures varies by \n",
    "     demographic factors (e.g., is the relationship stronger or weaker in different age groups or education levels?)\n",
    "   - This helps identify for whom the objective-subjective relationship is strongest or weakest\n",
    "\n",
    "For the coherence variable specifically:\n",
    "- Use regression to identify factors that predict greater or lesser coherence\n",
    "- This helps understand what leads some people to have better alignment between \n",
    "  objective and subjective measures than others\n",
    "\n",
    "These approaches will provide a comprehensive understanding of the factors influencing \n",
    "subjective memory evaluation and its alignment with objective cognitive measures.\n",
    "    \"\"\")\n",
    "\n",
    "# Call the function to suggest methods\n",
    "suggest_statistical_methods(df, ['memoria_subjetiva', 'coherencia'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Summarize the key findings from this notebook\n",
    "print(\"\\nSUMMARY OF KEY FINDINGS\\n\" + \"=\"*30)\n",
    "\n",
    "# Check what we have in the dataframe\n",
    "if 'minimental' in df.columns and 'memoria_subjetiva' in df.columns:\n",
    "    print(\"1. Numeric Variables Analysis:\")\n",
    "    print(f\"   - Identified {len(nonbinary_numeric)} non-binary numeric variables for analysis\")\n",
    "    \n",
    "    print(\"\\n2. Coherencia Variable Creation:\")\n",
    "    if 'coherencia' in df.columns:\n",
    "        print(f\"   - Created 'coherencia' variable to measure alignment between objective and subjective memory\")\n",
    "        print(f\"   - Mean coherencia value: {df['coherencia'].mean():.4f}\")\n",
    "        direction = \"positive\" if df['coherencia'].mean() > 0 else \"negative\"\n",
    "        print(f\"   - Overall {direction} coherence suggests {'better objective than subjective assessment' if direction == 'positive' else 'better subjective perception than objective performance'} on average\")\n",
    "    \n",
    "    print(\"\\n3. Correlation Analysis:\")\n",
    "    print(\"   - Identified key variables correlated with subjective memory and coherence\")\n",
    "    print(\"   - Correlation patterns suggest different factors influence subjective vs. objective memory\")\n",
    "    \n",
    "    print(\"\\n4. Statistical Methods:\")\n",
    "    print(\"   - Multiple approaches suggested for analyzing factors influencing memoria_subjetiva and coherence\")\n",
    "    print(\"   - Hierarchical regression, mediation/moderation analysis, and structural equation modeling are recommended approaches\")\n",
    "    \n",
    "    print(\"\\nNEXT STEPS:\")\n",
    "    print(\"1. Implement hierarchical regression models for memoria_subjetiva and coherencia\")\n",
    "    print(\"2. Conduct mediation analyses to understand pathways of influence\")\n",
    "    print(\"3. Test for moderation effects by demographic variables\")\n",
    "    print(\"4. Create visualizations of the key relationships for presentation\")\n",
    "    print(\"5. Save the enhanced dataset including the 'coherencia' variable for further analysis\")\n",
    "    \n",
    "    # Save the enhanced dataframe with the coherencia variable\n",
    "    df.to_csv('sabe_with_coherencia.csv', index=False)\n",
    "    print(\"\\nEnhanced dataset saved as 'sabe_with_coherencia.csv'\")\n",
    "else:\n",
    "    print(\"Unable to perform complete analysis due to missing key variables.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}